name: CI

# Required GitHub Secrets:
# - SOPS_AGE_KEY (required): Decrypt encrypted test environment files
# - CODECOV_TOKEN (required): Upload coverage reports to Codecov
# - SNYK_TOKEN (optional): Enable Snyk security scanning
# Verification: ./scripts/verify-github-secrets.sh

on:
  pull_request:
  push:
    branches: [main, develop]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  lint-and-type-check:
    name: Lint and Type Check
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js and dependencies
        uses: ./.github/actions/setup-node-app

      - name: Run ESLint
        run: npm run lint

      - name: Run type check
        run: npm run typecheck

      - name: Check code formatting
        run: npm run format:check

  unit-tests:
    name: Unit Tests
    needs: [lint-and-type-check]
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js and dependencies
        uses: ./.github/actions/setup-node-app

      # Required Secret: SOPS_AGE_KEY
      # Purpose: Decrypt .env.test.enc file for test execution
      - name: Setup SOPS
        uses: ./.github/actions/setup-sops
        with:
          sops-age-key: ${{ secrets.SOPS_AGE_KEY }}

      - name: Decrypt test secrets
        run: sops --decrypt .env.test.enc > .env.test

      # Run all unit tests with coverage thresholds enforced
      # NOTE: Only unit tests collect coverage for faster CI
      - name: Run unit tests with coverage
        run: npx vitest run --config vitest.config.unit-ci.ts --reporter=verbose --coverage

      - name: Cleanup decrypted secrets
        if: always()
        run: rm -f .env.test

      - name: Upload coverage
        uses: actions/upload-artifact@v4
        with:
          name: coverage-unit
          path: coverage/
          retention-days: 7

  # Integration, E2E, Chaos, and Performance tests run in parallel after unit tests pass
  integration-tests:
    name: Integration Tests
    needs: [unit-tests]
    runs-on: ubuntu-latest
    timeout-minutes: 15

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: test_db
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      rabbitmq:
        image: rabbitmq:3.12-management-alpine
        env:
          RABBITMQ_DEFAULT_USER: test
          RABBITMQ_DEFAULT_PASS: test
        options: >-
          --health-cmd "rabbitmq-diagnostics -q ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5672:5672
          - 15672:15672

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js and dependencies
        uses: ./.github/actions/setup-node-app

      - name: Setup SOPS
        uses: ./.github/actions/setup-sops
        with:
          sops-age-key: ${{ secrets.SOPS_AGE_KEY }}

      - name: Decrypt test secrets
        run: sops --decrypt .env.test.enc > .env.test

      - name: Wait for services
        uses: ./.github/actions/wait-for-services

      - name: Run database migrations
        run: npm run db:migrate
        env:
          DATABASE_URL: postgres://test:test@localhost:5432/test_db

      # OPTIMIZATION: Use optimized config WITHOUT coverage (coverage only from unit tests)
      - name: Run integration tests (optimized)
        run: npm run test:integration:optimized
        env:
          DATABASE_URL: postgres://test:test@localhost:5432/test_db
          RABBITMQ_URL: amqp://test:test@localhost:5672
          REDIS_URL: redis://localhost:6379
          ENABLE_DB_METRICS: 'false'
          CI: 'true'
          # Disable Ryuk (Testcontainers reaper) in CI as it has connectivity issues
          TESTCONTAINERS_RYUK_DISABLED: 'true'

      - name: Cleanup decrypted secrets
        if: always()
        run: rm -f .env.test

  e2e-tests:
    name: E2E Tests
    needs: [unit-tests]
    runs-on: ubuntu-latest
    timeout-minutes: 12  # Reduced from 20 with optimized config

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: test_db
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      rabbitmq:
        image: rabbitmq:3.12-management-alpine
        env:
          RABBITMQ_DEFAULT_USER: test
          RABBITMQ_DEFAULT_PASS: test
        options: >-
          --health-cmd "rabbitmq-diagnostics -q ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5672:5672
          - 15672:15672

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js and dependencies
        uses: ./.github/actions/setup-node-app

      - name: Setup SOPS
        uses: ./.github/actions/setup-sops
        with:
          sops-age-key: ${{ secrets.SOPS_AGE_KEY }}

      - name: Decrypt test secrets
        run: sops --decrypt .env.test.enc > .env.test

      - name: Wait for services
        uses: ./.github/actions/wait-for-services

      - name: Run database migrations
        run: npm run db:migrate
        env:
          DATABASE_URL: postgres://test:test@localhost:5432/test_db

      # OPTIMIZATION: Use optimized E2E config WITHOUT coverage (coverage only from unit tests)
      - name: Run E2E tests (optimized)
        run: npm run test:e2e:optimized
        env:
          DATABASE_URL: postgres://test:test@localhost:5432/test_db
          RABBITMQ_URL: amqp://test:test@localhost:5672
          REDIS_URL: redis://localhost:6379
          API_URL: http://localhost:3000
          ENABLE_DB_METRICS: 'false'
          CI: 'true'
          # Disable Ryuk (Testcontainers reaper) in CI as it has connectivity issues
          TESTCONTAINERS_RYUK_DISABLED: 'true'

      - name: Cleanup decrypted secrets
        if: always()
        run: rm -f .env.test

  chaos-tests:
    name: Chaos Tests
    needs: [unit-tests]
    runs-on: ubuntu-latest
    timeout-minutes: 20

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: test_db
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      rabbitmq:
        image: rabbitmq:3.12-management-alpine
        env:
          RABBITMQ_DEFAULT_USER: test
          RABBITMQ_DEFAULT_PASS: test
        options: >-
          --health-cmd "rabbitmq-diagnostics -q ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5672:5672
          - 15672:15672

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js and dependencies
        uses: ./.github/actions/setup-node-app

      - name: Setup SOPS
        uses: ./.github/actions/setup-sops
        with:
          sops-age-key: ${{ secrets.SOPS_AGE_KEY }}

      - name: Decrypt test secrets
        run: sops --decrypt .env.test.enc > .env.test

      - name: Wait for services
        uses: ./.github/actions/wait-for-services
        with:
          redis: false

      - name: Run database migrations
        run: npm run db:migrate
        env:
          DATABASE_URL: postgres://test:test@localhost:5432/test_db

      - name: Run chaos tests
        run: npm run test:chaos
        env:
          DATABASE_URL: postgres://test:test@localhost:5432/test_db
          RABBITMQ_URL: amqp://test:test@localhost:5672
          ENABLE_DB_METRICS: 'false'
          CI: true
          # Disable Ryuk (Testcontainers reaper) in CI as it has connectivity issues
          TESTCONTAINERS_RYUK_DISABLED: 'true'

      - name: Cleanup decrypted secrets
        if: always()
        run: rm -f .env.test

  performance-smoke-test:
    name: Performance Smoke Test
    needs: [unit-tests]
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js and dependencies
        uses: ./.github/actions/setup-node-app

      - name: Setup SOPS
        uses: ./.github/actions/setup-sops
        with:
          sops-age-key: ${{ secrets.SOPS_AGE_KEY }}

      - name: Decrypt test secrets
        run: sops --decrypt .env.test.enc > .env.test

      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6

      - name: Start test environment
        run: docker compose -f docker-compose.test.yml up -d

      - name: Wait for services
        run: |
          timeout 120 bash -c 'until docker compose -f docker-compose.test.yml exec -T postgres pg_isready -U test -d test_db; do sleep 2; done'
          timeout 120 bash -c 'until docker compose -f docker-compose.test.yml exec -T rabbitmq rabbitmq-diagnostics -q ping; do sleep 2; done'
          timeout 120 bash -c 'until docker compose -f docker-compose.test.yml exec -T redis redis-cli ping; do sleep 2; done'

      - name: Run database migrations
        run: npm run db:migrate
        env:
          DATABASE_URL: postgres://test:test@localhost:5432/test_db

      - name: Start API server
        run: |
          npm run build
          node dist/src/index.js > /tmp/api.log 2>&1 &
          echo $! > /tmp/api.pid
          echo "API server started with PID $(cat /tmp/api.pid)"
          sleep 15
        env:
          DATABASE_URL: postgres://test:test@localhost:5432/test_db
          DATABASE_HOST: localhost
          DATABASE_PORT: 5432
          DATABASE_USER: test
          DATABASE_PASSWORD: test
          DATABASE_NAME: test_db
          RABBITMQ_URL: amqp://test:test@localhost:5672
          RABBITMQ_HOST: localhost
          RABBITMQ_PORT: 5672
          RABBITMQ_USER: test
          RABBITMQ_PASSWORD: test
          REDIS_URL: redis://localhost:6379
          HOST: 0.0.0.0
          PORT: 3000
          NODE_ENV: test
          # Higher rate limits for performance testing
          RATE_LIMIT_CREATE_USER_MAX: 10000
          RATE_LIMIT_READ_USER_MAX: 10000
          RATE_LIMIT_UPDATE_USER_MAX: 10000
          RATE_LIMIT_DELETE_USER_MAX: 10000

      - name: Wait for API
        run: |
          echo "Waiting for API to be ready..."
          echo "Checking if process is running..."
          ps aux | grep node || true
          echo "Checking if port 3000 is listening..."
          ss -tlnp | grep 3000 || netstat -tlnp 2>/dev/null | grep 3000 || echo "Port 3000 not found in listeners"

          for i in {1..30}; do
            echo "Attempt $i/30 at $(date +%H:%M:%S)..."

            # Try curl without silent mode to see errors
            RESPONSE=$(curl -s --max-time 5 http://localhost:3000/health 2>&1) || CURL_EXIT=$?
            echo "Curl response: $RESPONSE"
            echo "Curl exit code: ${CURL_EXIT:-0}"

            if echo "$RESPONSE" | grep -q "ok"; then
              echo "API is ready!"
              exit 0
            fi

            # Check if process is still running
            if [ -f /tmp/api.pid ]; then
              PID=$(cat /tmp/api.pid)
              if ! ps -p $PID > /dev/null 2>&1; then
                echo "ERROR: API process $PID is no longer running!"
                echo "=== API Server Logs ==="
                cat /tmp/api.log || echo "No logs found"
                exit 1
              fi
            fi

            sleep 3
          done

          echo "API failed to start within timeout!"
          echo "=== Final Process Check ==="
          ps aux | grep node || true
          echo "=== Port Check ==="
          ss -tlnp | grep 3000 || netstat -tlnp 2>/dev/null | grep 3000 || true
          echo "=== API Server Logs ==="
          cat /tmp/api.log || echo "No logs found"
          exit 1

      - name: Create results directory
        run: mkdir -p perf-results

      - name: Run performance smoke test
        run: k6 run tests/performance/api-smoke.test.js
        env:
          API_URL: http://localhost:3000

      - name: Stop API server
        if: always()
        run: |
          if [ -f /tmp/api.pid ]; then
            kill $(cat /tmp/api.pid) || true
          fi

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-smoke-results
          path: perf-results/
          retention-days: 7

      - name: Cleanup decrypted secrets
        if: always()
        run: rm -f .env.test

      - name: Cleanup
        if: always()
        run: docker compose -f docker-compose.test.yml down -v

  performance-load-tests:
    name: Performance Load Tests (${{ matrix.test }})
    needs: [unit-tests]
    runs-on: ubuntu-latest
    timeout-minutes: 12  # Reduced from 15
    strategy:
      fail-fast: false
      matrix:
        # OPTIMIZATION: Only run api-load test which uses public endpoints
        # scheduler-load, worker-throughput, e2e-load require internal endpoints that don't exist
        test: [api-load]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js and dependencies
        uses: ./.github/actions/setup-node-app

      - name: Setup SOPS
        uses: ./.github/actions/setup-sops
        with:
          sops-age-key: ${{ secrets.SOPS_AGE_KEY }}

      - name: Decrypt test secrets
        run: sops --decrypt .env.test.enc > .env.test

      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6

      - name: Start test environment
        run: docker compose -f docker-compose.test.yml up -d

      - name: Wait for services
        run: |
          timeout 120 bash -c 'until docker compose -f docker-compose.test.yml exec -T postgres pg_isready -U test -d test_db; do sleep 2; done'
          timeout 120 bash -c 'until docker compose -f docker-compose.test.yml exec -T rabbitmq rabbitmq-diagnostics -q ping; do sleep 2; done'
          timeout 120 bash -c 'until docker compose -f docker-compose.test.yml exec -T redis redis-cli ping; do sleep 2; done'

      - name: Run database migrations
        run: npm run db:migrate
        env:
          DATABASE_URL: postgres://test:test@localhost:5432/test_db

      - name: Start API server
        run: |
          npm run build
          node dist/src/index.js > /tmp/api.log 2>&1 &
          echo $! > /tmp/api.pid
          echo "API server started with PID $(cat /tmp/api.pid)"
          sleep 15
        env:
          DATABASE_URL: postgres://test:test@localhost:5432/test_db
          DATABASE_HOST: localhost
          DATABASE_PORT: 5432
          DATABASE_USER: test
          DATABASE_PASSWORD: test
          DATABASE_NAME: test_db
          RABBITMQ_URL: amqp://test:test@localhost:5672
          RABBITMQ_HOST: localhost
          RABBITMQ_PORT: 5672
          RABBITMQ_USER: test
          RABBITMQ_PASSWORD: test
          REDIS_URL: redis://localhost:6379
          HOST: 0.0.0.0
          PORT: 3000
          NODE_ENV: test
          # Higher rate limits for performance testing
          RATE_LIMIT_CREATE_USER_MAX: 10000
          RATE_LIMIT_READ_USER_MAX: 10000
          RATE_LIMIT_UPDATE_USER_MAX: 10000
          RATE_LIMIT_DELETE_USER_MAX: 10000

      - name: Wait for API
        run: |
          echo "Waiting for API to be ready..."
          echo "Checking if process is running..."
          ps aux | grep node || true
          echo "Checking if port 3000 is listening..."
          ss -tlnp | grep 3000 || netstat -tlnp 2>/dev/null | grep 3000 || echo "Port 3000 not found in listeners"

          for i in {1..30}; do
            echo "Attempt $i/30 at $(date +%H:%M:%S)..."

            # Try curl without silent mode to see errors
            RESPONSE=$(curl -s --max-time 5 http://localhost:3000/health 2>&1) || CURL_EXIT=$?
            echo "Curl response: $RESPONSE"
            echo "Curl exit code: ${CURL_EXIT:-0}"

            if echo "$RESPONSE" | grep -q "ok"; then
              echo "API is ready!"
              exit 0
            fi

            # Check if process is still running
            if [ -f /tmp/api.pid ]; then
              PID=$(cat /tmp/api.pid)
              if ! ps -p $PID > /dev/null 2>&1; then
                echo "ERROR: API process $PID is no longer running!"
                echo "=== API Server Logs ==="
                cat /tmp/api.log || echo "No logs found"
                exit 1
              fi
            fi

            sleep 3
          done

          echo "API failed to start within timeout!"
          echo "=== Final Process Check ==="
          ps aux | grep node || true
          echo "=== Port Check ==="
          ss -tlnp | grep 3000 || netstat -tlnp 2>/dev/null | grep 3000 || true
          echo "=== API Server Logs ==="
          cat /tmp/api.log || echo "No logs found"
          exit 1

      - name: Create results directory
        run: mkdir -p perf-results

      - name: Run performance test
        id: perf-test
        run: k6 run tests/performance/${{ matrix.test }}.test.js --out json=perf-results/${{ matrix.test }}.json
        env:
          API_URL: http://localhost:3000

      - name: Performance Test Summary
        if: always()
        run: |
          echo "## Performance Test Results - ${{ matrix.test }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          test_outcome="${{ steps.perf-test.outcome }}"

          if [ "$test_outcome" == "failure" ]; then
            echo "- :x: ${{ matrix.test }} **FAILED**" >> $GITHUB_STEP_SUMMARY
            echo "::warning::${{ matrix.test }} performance test failed - review results before merging"
          elif [ "$test_outcome" == "success" ]; then
            echo "- :white_check_mark: ${{ matrix.test }} PASSED" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Stop API server
        if: always()
        run: |
          if [ -f /tmp/api.pid ]; then
            kill $(cat /tmp/api.pid) || true
          fi

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-load-results-${{ matrix.test }}
          path: perf-results/
          retention-days: 30

      - name: Cleanup decrypted secrets
        if: always()
        run: rm -f .env.test

      - name: Cleanup
        if: always()
        run: docker compose -f docker-compose.test.yml down -v

  coverage-report:
    name: Coverage Report
    # OPTIMIZATION: Only depends on unit-tests which generates coverage
    # Integration and E2E tests run without coverage for speed
    needs: [unit-tests]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Validate Codecov Token
        id: codecov-check
        run: |
          if [ -z "${{ secrets.CODECOV_TOKEN }}" ]; then
            echo "::warning::CODECOV_TOKEN not configured. Coverage upload will be skipped."
            echo "To enable Codecov integration:"
            echo "  1. Get your token from https://codecov.io/"
            echo "  2. Run: gh secret set CODECOV_TOKEN"
            echo "skip=true" >> $GITHUB_OUTPUT
          else
            echo "CODECOV_TOKEN is configured"
            echo "skip=false" >> $GITHUB_OUTPUT
          fi

      - name: Download all coverage artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: coverage-*
          path: coverage-artifacts/

      - name: Setup Node.js and dependencies
        uses: ./.github/actions/setup-node-app

      - name: Merge coverage reports
        run: |
          mkdir -p coverage

          # Merge V8 coverage files from all test types using a simple Node script
          node -e "
          const fs = require('fs');
          const path = require('path');

          // Find all coverage-final.json files recursively
          function findCoverageFiles(dir) {
            const files = [];
            try {
              const entries = fs.readdirSync(dir, { withFileTypes: true });
              for (const entry of entries) {
                const fullPath = path.join(dir, entry.name);
                if (entry.isDirectory()) {
                  files.push(...findCoverageFiles(fullPath));
                } else if (entry.name === 'coverage-final.json') {
                  files.push(fullPath);
                }
              }
            } catch (e) { /* ignore */ }
            return files;
          }

          const merged = {};
          const coverageFiles = findCoverageFiles('coverage-artifacts');

          for (const file of coverageFiles) {
            try {
              const coverage = JSON.parse(fs.readFileSync(file, 'utf8'));
              // Merge coverage data, preferring higher coverage counts
              for (const [filePath, data] of Object.entries(coverage)) {
                if (!merged[filePath]) {
                  merged[filePath] = data;
                } else {
                  // Merge statement, function, and branch counts
                  const existing = merged[filePath];
                  if (data.s) {
                    for (const key of Object.keys(data.s)) {
                      existing.s[key] = Math.max(existing.s[key] || 0, data.s[key] || 0);
                    }
                  }
                  if (data.f) {
                    for (const key of Object.keys(data.f)) {
                      existing.f[key] = Math.max(existing.f[key] || 0, data.f[key] || 0);
                    }
                  }
                  if (data.b) {
                    for (const key of Object.keys(data.b)) {
                      existing.b[key] = existing.b[key].map((v, i) => Math.max(v, data.b[key][i] || 0));
                    }
                  }
                }
              }
            } catch (e) {
              console.error('Error processing', file, e.message);
            }
          }

          fs.writeFileSync('coverage/coverage-final.json', JSON.stringify(merged, null, 2));
          console.log('Merged', Object.keys(merged).length, 'files from', coverageFiles.length, 'reports');

          // Generate coverage-summary.json from Istanbul format data
          let totalStatements = 0, coveredStatements = 0;
          let totalBranches = 0, coveredBranches = 0;
          let totalFunctions = 0, coveredFunctions = 0;

          for (const [filePath, data] of Object.entries(merged)) {
            if (data.s) {
              for (const [key, count] of Object.entries(data.s)) {
                totalStatements++;
                if (count > 0) coveredStatements++;
              }
            }
            if (data.b) {
              for (const [key, counts] of Object.entries(data.b)) {
                for (const count of counts) {
                  totalBranches++;
                  if (count > 0) coveredBranches++;
                }
              }
            }
            if (data.f) {
              for (const [key, count] of Object.entries(data.f)) {
                totalFunctions++;
                if (count > 0) coveredFunctions++;
              }
            }
          }

          const pct = (c, t) => t === 0 ? 100 : Math.round((c / t) * 10000) / 100;
          const summary = {
            total: {
              lines: { total: totalStatements, covered: coveredStatements, skipped: 0, pct: pct(coveredStatements, totalStatements) },
              statements: { total: totalStatements, covered: coveredStatements, skipped: 0, pct: pct(coveredStatements, totalStatements) },
              functions: { total: totalFunctions, covered: coveredFunctions, skipped: 0, pct: pct(coveredFunctions, totalFunctions) },
              branches: { total: totalBranches, covered: coveredBranches, skipped: 0, pct: pct(coveredBranches, totalBranches) }
            }
          };

          fs.writeFileSync('coverage/coverage-summary.json', JSON.stringify(summary, null, 2));
          console.log('Coverage:', summary.total.statements.pct + '% statements,', summary.total.functions.pct + '% functions,', summary.total.branches.pct + '% branches');
          "

      # CRITICAL: Coverage Enforcement
      # This step FAILS CI if merged coverage drops below thresholds:
      # - Lines: 80%, Functions: 50%, Branches: 75%, Statements: 80%
      # Thresholds defined in: scripts/coverage/check-thresholds.sh
      # Must match: vitest.config.base.ts
      - name: Check coverage threshold
        run: bash scripts/coverage/check-thresholds.sh coverage/coverage-summary.json

      # Required Secret: CODECOV_TOKEN
      # Purpose: Upload coverage reports to Codecov for tracking and analysis
      # Setup: Get token from https://codecov.io/ and run 'gh secret set CODECOV_TOKEN'
      # Note: This step is skipped if CODECOV_TOKEN is not configured (graceful degradation)
      - name: Upload merged coverage to Codecov
        if: steps.codecov-check.outputs.skip != 'true'
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage/lcov.info
          flags: unittests,integration,e2e
          name: codecov-umbrella
          fail_ci_if_error: true
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

      - name: Update coverage history
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          if [ -f scripts/coverage/update-history.sh ]; then
            bash scripts/coverage/update-history.sh
          fi

      - name: Comment coverage on PR
        if: github.event_name == 'pull_request'
        uses: romeovs/lcov-reporter-action@v0.3.1
        with:
          lcov-file: ./coverage/lcov.info
          github-token: ${{ secrets.GITHUB_TOKEN }}
          delete-old-comments: true

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js and dependencies
        uses: ./.github/actions/setup-node-app
        with:
          install-dependencies: 'false'

      - name: Run npm audit
        run: npm audit --audit-level=critical

      # Optional Secret: SNYK_TOKEN
      # Purpose: Enable Snyk vulnerability scanning for dependencies
      # Setup: Get token from https://snyk.io/ and run 'gh secret set SNYK_TOKEN'
      # Note: Step is skipped if token is missing (graceful degradation)
      - name: Run Snyk security scan
        if: env.SNYK_TOKEN != ''
        uses: snyk/actions/node@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          args: --severity-threshold=high

  build:
    name: Build
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js and dependencies
        uses: ./.github/actions/setup-node-app

      - name: Build application
        run: npm run build

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist
          path: dist/
          retention-days: 7

  all-checks-passed:
    name: All Checks Passed
    needs:
      - lint-and-type-check
      - unit-tests
      - integration-tests
      - e2e-tests
      - chaos-tests
      - performance-smoke-test
      - performance-load-tests
      - coverage-report
      - security-scan
      - build
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Check all jobs
        run: |
          # Check core required jobs
          FAILED=""
          if [[ "${{ needs.lint-and-type-check.result }}" != "success" ]]; then FAILED="$FAILED lint-and-type-check"; fi
          if [[ "${{ needs.unit-tests.result }}" != "success" ]]; then FAILED="$FAILED unit-tests"; fi
          if [[ "${{ needs.integration-tests.result }}" != "success" ]]; then FAILED="$FAILED integration-tests"; fi
          if [[ "${{ needs.e2e-tests.result }}" != "success" ]]; then FAILED="$FAILED e2e-tests"; fi
          if [[ "${{ needs.build.result }}" != "success" ]]; then FAILED="$FAILED build"; fi

          # Check optional jobs (don't fail CI if these fail, but warn)
          if [[ "${{ needs.chaos-tests.result }}" != "success" ]]; then echo "::warning::chaos-tests did not pass"; fi
          if [[ "${{ needs.performance-smoke-test.result }}" != "success" ]]; then echo "::warning::performance-smoke-test did not pass"; fi
          if [[ "${{ needs.performance-load-tests.result }}" != "success" ]]; then echo "::warning::performance-load-tests did not pass"; fi
          if [[ "${{ needs.coverage-report.result }}" != "success" ]]; then echo "::warning::coverage-report did not pass"; fi

          if [[ -n "$FAILED" ]]; then
            echo "Required checks failed:$FAILED"
            exit 1
          fi
          echo "All required checks passed!"
